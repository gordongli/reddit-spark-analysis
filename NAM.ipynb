{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up our Schema\n",
    "\n",
    "Spark can automatically create a schema for CSV files, but ours don't have headings. Let's set this up here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Timestamp,LongType,true),StructField(Geohash,StringType,true),StructField(geopotential_height_lltw,FloatType,true),StructField(water_equiv_of_accum_snow_depth_surface,FloatType,true),StructField(drag_coefficient_surface,FloatType,true),StructField(sensible_heat_net_flux_surface,FloatType,true),StructField(categorical_ice_pellets_yes1_no0_surface,FloatType,true),StructField(visibility_surface,FloatType,true),StructField(number_of_soil_layers_in_root_zone_surface,FloatType,true),StructField(categorical_freezing_rain_yes1_no0_surface,FloatType,true),StructField(pressure_reduced_to_msl_msl,FloatType,true),StructField(upward_short_wave_rad_flux_surface,FloatType,true),StructField(relative_humidity_zerodegc_isotherm,FloatType,true),StructField(categorical_snow_yes1_no0_surface,FloatType,true),StructField(u-component_of_wind_tropopause,FloatType,true),StructField(surface_wind_gust_surface,FloatType,true),StructField(total_cloud_cover_entire_atmosphere,FloatType,true),StructField(upward_long_wave_rad_flux_surface,FloatType,true),StructField(land_cover_land1_sea0_surface,FloatType,true),StructField(vegitation_type_as_in_sib_surface,FloatType,true),StructField(v-component_of_wind_pblri,FloatType,true),StructField(albedo_surface,FloatType,true),StructField(lightning_surface,FloatType,true),StructField(ice_cover_ice1_no_ice0_surface,FloatType,true),StructField(convective_inhibition_surface,FloatType,true),StructField(pressure_surface,FloatType,true),StructField(transpiration_stress-onset_soil_moisture_surface,FloatType,true),StructField(soil_porosity_surface,FloatType,true),StructField(vegetation_surface,FloatType,true),StructField(categorical_rain_yes1_no0_surface,FloatType,true),StructField(downward_long_wave_rad_flux_surface,FloatType,true),StructField(planetary_boundary_layer_height_surface,FloatType,true),StructField(soil_type_as_in_zobler_surface,FloatType,true),StructField(geopotential_height_cloud_base,FloatType,true),StructField(friction_velocity_surface,FloatType,true),StructField(maximumcomposite_radar_reflectivity_entire_atmosphere,FloatType,true),StructField(plant_canopy_surface_water_surface,FloatType,true),StructField(v-component_of_wind_maximum_wind,FloatType,true),StructField(geopotential_height_zerodegc_isotherm,FloatType,true),StructField(mean_sea_level_pressure_nam_model_reduction_msl,FloatType,true),StructField(temperature_surface,FloatType,true),StructField(snow_cover_surface,FloatType,true),StructField(geopotential_height_surface,FloatType,true),StructField(convective_available_potential_energy_surface,FloatType,true),StructField(latent_heat_net_flux_surface,FloatType,true),StructField(surface_roughness_surface,FloatType,true),StructField(pressure_maximum_wind,FloatType,true),StructField(temperature_tropopause,FloatType,true),StructField(geopotential_height_pblri,FloatType,true),StructField(pressure_tropopause,FloatType,true),StructField(snow_depth_surface,FloatType,true),StructField(v-component_of_wind_tropopause,FloatType,true),StructField(downward_short_wave_rad_flux_surface,FloatType,true),StructField(u-component_of_wind_maximum_wind,FloatType,true),StructField(wilting_point_surface,FloatType,true),StructField(precipitable_water_entire_atmosphere,FloatType,true),StructField(u-component_of_wind_pblri,FloatType,true),StructField(direct_evaporation_cease_soil_moisture_surface,FloatType,true)))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, FloatType, LongType, StringType\n",
    "\n",
    "feats = []\n",
    "f = open('features.txt')\n",
    "for line_num, line in enumerate(f):\n",
    "    if line_num == 0:\n",
    "        # Timestamp\n",
    "        feats.append(StructField(line.strip(), LongType(), True))\n",
    "    elif line_num == 1:\n",
    "        # Geohash\n",
    "        feats.append(StructField(line.strip(), StringType(), True))\n",
    "    else:\n",
    "        # Other features\n",
    "        feats.append(StructField(line.strip(), FloatType(), True))\n",
    "    \n",
    "schema = StructType(feats)\n",
    "\n",
    "print(schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dataframe\n",
    "\n",
    "Let's load our CSV into a 'dataframe' - Spark's abstraction for working with tabular data (built on top of RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam_t = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:15000/nam_tiny.tdv')\n",
    "nam_s = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:15000/nam_s/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Timestamp: bigint, Geohash: string, geopotential_height_lltw: float, water_equiv_of_accum_snow_depth_surface: float, drag_coefficient_surface: float, sensible_heat_net_flux_surface: float, categorical_ice_pellets_yes1_no0_surface: float, visibility_surface: float, number_of_soil_layers_in_root_zone_surface: float, categorical_freezing_rain_yes1_no0_surface: float, pressure_reduced_to_msl_msl: float, upward_short_wave_rad_flux_surface: float, relative_humidity_zerodegc_isotherm: float, categorical_snow_yes1_no0_surface: float, u-component_of_wind_tropopause: float, surface_wind_gust_surface: float, total_cloud_cover_entire_atmosphere: float, upward_long_wave_rad_flux_surface: float, land_cover_land1_sea0_surface: float, vegitation_type_as_in_sib_surface: float, v-component_of_wind_pblri: float, albedo_surface: float, lightning_surface: float, ice_cover_ice1_no_ice0_surface: float, convective_inhibition_surface: float, pressure_surface: float, transpiration_stress-onset_soil_moisture_surface: float, soil_porosity_surface: float, vegetation_surface: float, categorical_rain_yes1_no0_surface: float, downward_long_wave_rad_flux_surface: float, planetary_boundary_layer_height_surface: float, soil_type_as_in_zobler_surface: float, geopotential_height_cloud_base: float, friction_velocity_surface: float, maximumcomposite_radar_reflectivity_entire_atmosphere: float, plant_canopy_surface_water_surface: float, v-component_of_wind_maximum_wind: float, geopotential_height_zerodegc_isotherm: float, mean_sea_level_pressure_nam_model_reduction_msl: float, temperature_surface: float, snow_cover_surface: float, geopotential_height_surface: float, convective_available_potential_energy_surface: float, latent_heat_net_flux_surface: float, surface_roughness_surface: float, pressure_maximum_wind: float, temperature_tropopause: float, geopotential_height_pblri: float, pressure_tropopause: float, snow_depth_surface: float, v-component_of_wind_tropopause: float, downward_short_wave_rad_flux_surface: float, u-component_of_wind_maximum_wind: float, wilting_point_surface: float, precipitable_water_entire_atmosphere: float, u-component_of_wind_pblri: float, direct_evaporation_cease_soil_moisture_surface: float]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nam_t.cache()\n",
    "nam_s.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Timestamp: bigint, Geohash: string, geopotential_height_lltw: float, water_equiv_of_accum_snow_depth_surface: float, drag_coefficient_surface: float, sensible_heat_net_flux_surface: float, categorical_ice_pellets_yes1_no0_surface: float, visibility_surface: float, number_of_soil_layers_in_root_zone_surface: float, categorical_freezing_rain_yes1_no0_surface: float, pressure_reduced_to_msl_msl: float, upward_short_wave_rad_flux_surface: float, relative_humidity_zerodegc_isotherm: float, categorical_snow_yes1_no0_surface: float, u-component_of_wind_tropopause: float, surface_wind_gust_surface: float, total_cloud_cover_entire_atmosphere: float, upward_long_wave_rad_flux_surface: float, land_cover_land1_sea0_surface: float, vegitation_type_as_in_sib_surface: float, v-component_of_wind_pblri: float, albedo_surface: float, lightning_surface: float, ice_cover_ice1_no_ice0_surface: float, convective_inhibition_surface: float, pressure_surface: float, transpiration_stress-onset_soil_moisture_surface: float, soil_porosity_surface: float, vegetation_surface: float, categorical_rain_yes1_no0_surface: float, downward_long_wave_rad_flux_surface: float, planetary_boundary_layer_height_surface: float, soil_type_as_in_zobler_surface: float, geopotential_height_cloud_base: float, friction_velocity_surface: float, maximumcomposite_radar_reflectivity_entire_atmosphere: float, plant_canopy_surface_water_surface: float, v-component_of_wind_maximum_wind: float, geopotential_height_zerodegc_isotherm: float, mean_sea_level_pressure_nam_model_reduction_msl: float, temperature_surface: float, snow_cover_surface: float, geopotential_height_surface: float, convective_available_potential_energy_surface: float, latent_heat_net_flux_surface: float, surface_roughness_surface: float, pressure_maximum_wind: float, temperature_tropopause: float, geopotential_height_pblri: float, pressure_tropopause: float, snow_depth_surface: float, v-component_of_wind_tropopause: float, downward_short_wave_rad_flux_surface: float, u-component_of_wind_maximum_wind: float, wilting_point_surface: float, precipitable_water_entire_atmosphere: float, u-component_of_wind_pblri: float, direct_evaporation_cease_soil_moisture_surface: float]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nam = spark.read.format('csv').option('sep', '\\t').schema(schema).load('hdfs://orion11:15000/nam/*')\n",
    "nam.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an SQL 'table'\n",
    "nam_t.createOrReplaceTempView(\"nam_t\")\n",
    "nam_s.createOrReplaceTempView(\"nam_s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.cacheTable(\"nam_t\")\n",
    "spark.catalog.cacheTable(\"nam_s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nam.createOrReplaceTempView(\"nam\")\n",
    "spark.catalog.cacheTable(\"nam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unknown Feature\n",
    "I didn't know what albedo was, so I looked at its summary statistics. Still unsure, I looked up the definition: 'the proportion of the incident light or radiation that is reflected by a surface, typically that of a planet or moon.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|summary|  albedo_surface|\n",
      "+-------+----------------+\n",
      "|  count|             100|\n",
      "|   mean|           18.07|\n",
      "| stddev|17.4802948221907|\n",
      "|    min|             6.0|\n",
      "|    max|            76.0|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nam_t.describe('albedo_surface').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hot Hot Hot\n",
    "The hottest tempurature in the dataset is 330.67431640625 at location d5f0jqerq27bat (21.13070154, -86.9520505; Benito Ju√°rez, Quintana Roo, Mexico) at time 2015-08-23T18:00Z. This record is not surprising, as it is near the equator in the summer. Looking at the other highest tempuratures which are near this highest one, it does not appear to be an anomaly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Timestamp: bigint, Geohash: string, geopotential_height_lltw: float, water_equiv_of_accum_snow_depth_surface: float, drag_coefficient_surface: float, sensible_heat_net_flux_surface: float, categorical_ice_pellets_yes1_no0_surface: float, visibility_surface: float, number_of_soil_layers_in_root_zone_surface: float, categorical_freezing_rain_yes1_no0_surface: float, pressure_reduced_to_msl_msl: float, upward_short_wave_rad_flux_surface: float, relative_humidity_zerodegc_isotherm: float, categorical_snow_yes1_no0_surface: float, u-component_of_wind_tropopause: float, surface_wind_gust_surface: float, total_cloud_cover_entire_atmosphere: float, upward_long_wave_rad_flux_surface: float, land_cover_land1_sea0_surface: float, vegitation_type_as_in_sib_surface: float, v-component_of_wind_pblri: float, albedo_surface: float, lightning_surface: float, ice_cover_ice1_no_ice0_surface: float, convective_inhibition_surface: float, pressure_surface: float, transpiration_stress-onset_soil_moisture_surface: float, soil_porosity_surface: float, vegetation_surface: float, categorical_rain_yes1_no0_surface: float, downward_long_wave_rad_flux_surface: float, planetary_boundary_layer_height_surface: float, soil_type_as_in_zobler_surface: float, geopotential_height_cloud_base: float, friction_velocity_surface: float, maximumcomposite_radar_reflectivity_entire_atmosphere: float, plant_canopy_surface_water_surface: float, v-component_of_wind_maximum_wind: float, geopotential_height_zerodegc_isotherm: float, mean_sea_level_pressure_nam_model_reduction_msl: float, temperature_surface: float, snow_cover_surface: float, geopotential_height_surface: float, convective_available_potential_energy_surface: float, latent_heat_net_flux_surface: float, surface_roughness_surface: float, pressure_maximum_wind: float, temperature_tropopause: float, geopotential_height_pblri: float, pressure_tropopause: float, snow_depth_surface: float, v-component_of_wind_tropopause: float, downward_short_wave_rad_flux_surface: float, u-component_of_wind_maximum_wind: float, wilting_point_surface: float, precipitable_water_entire_atmosphere: float, u-component_of_wind_pblri: float, direct_evaporation_cease_soil_moisture_surface: float]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_desc = nam_s.orderBy(\"temperature_surface\", ascending=False)\n",
    "temp_desc.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hottest_record = temp_desc.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330.67431640625"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hottest_record['temperature_surface']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2015, 8, 23, 18, 0, tzinfo=datetime.timezone.utc)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "timestamp = hottest_record['Timestamp'] / 1000\n",
    "datetime.fromtimestamp(timestamp, timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d5f0jqerq27b'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hottest_record['Geohash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(temperature_surface=330.67431640625, Timestamp=1440352800000, Geohash='d5f0jqerq27b'),\n",
       " Row(temperature_surface=330.640625, Timestamp=1440266400000, Geohash='d5f0vd8eb80p'),\n",
       " Row(temperature_surface=330.6044921875, Timestamp=1430157600000, Geohash='9g77js659k20'),\n",
       " Row(temperature_surface=330.53662109375, Timestamp=1439056800000, Geohash='d5f0jqerq27b'),\n",
       " Row(temperature_surface=330.48193359375, Timestamp=1440612000000, Geohash='d59d5yttuc5b'),\n",
       " Row(temperature_surface=330.35693359375, Timestamp=1440612000000, Geohash='d59eqv7e03pb'),\n",
       " Row(temperature_surface=330.23193359375, Timestamp=1440612000000, Geohash='d59dntd726gz'),\n",
       " Row(temperature_surface=330.220703125, Timestamp=1440698400000, Geohash='d59eqv7e03pb'),\n",
       " Row(temperature_surface=330.179931640625, Timestamp=1438279200000, Geohash='d5f04xyhucez'),\n",
       " Row(temperature_surface=330.14990234375, Timestamp=1439488800000, Geohash='d5dpds10m55b')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_desc.select(\"temperature_surface\", \"Timestamp\", \"Geohash\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Timestamp: bigint, Geohash: string, geopotential_height_lltw: float, water_equiv_of_accum_snow_depth_surface: float, drag_coefficient_surface: float, sensible_heat_net_flux_surface: float, categorical_ice_pellets_yes1_no0_surface: float, visibility_surface: float, number_of_soil_layers_in_root_zone_surface: float, categorical_freezing_rain_yes1_no0_surface: float, pressure_reduced_to_msl_msl: float, upward_short_wave_rad_flux_surface: float, relative_humidity_zerodegc_isotherm: float, categorical_snow_yes1_no0_surface: float, u-component_of_wind_tropopause: float, surface_wind_gust_surface: float, total_cloud_cover_entire_atmosphere: float, upward_long_wave_rad_flux_surface: float, land_cover_land1_sea0_surface: float, vegitation_type_as_in_sib_surface: float, v-component_of_wind_pblri: float, albedo_surface: float, lightning_surface: float, ice_cover_ice1_no_ice0_surface: float, convective_inhibition_surface: float, pressure_surface: float, transpiration_stress-onset_soil_moisture_surface: float, soil_porosity_surface: float, vegetation_surface: float, categorical_rain_yes1_no0_surface: float, downward_long_wave_rad_flux_surface: float, planetary_boundary_layer_height_surface: float, soil_type_as_in_zobler_surface: float, geopotential_height_cloud_base: float, friction_velocity_surface: float, maximumcomposite_radar_reflectivity_entire_atmosphere: float, plant_canopy_surface_water_surface: float, v-component_of_wind_maximum_wind: float, geopotential_height_zerodegc_isotherm: float, mean_sea_level_pressure_nam_model_reduction_msl: float, temperature_surface: float, snow_cover_surface: float, geopotential_height_surface: float, convective_available_potential_energy_surface: float, latent_heat_net_flux_surface: float, surface_roughness_surface: float, pressure_maximum_wind: float, temperature_tropopause: float, geopotential_height_pblri: float, pressure_tropopause: float, snow_depth_surface: float, v-component_of_wind_tropopause: float, downward_short_wave_rad_flux_surface: float, u-component_of_wind_maximum_wind: float, wilting_point_surface: float, precipitable_water_entire_atmosphere: float, u-component_of_wind_pblri: float, direct_evaporation_cease_soil_moisture_surface: float]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_desc.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So Snowy\n",
    "The location where it snows most often is c41uhb4r5n00 (56.9543589, -132.32710345; City and Borough of Wrangell, AK). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[geohash: string, sum: double, cnt: bigint, div: double]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "so_snowy_s = spark.sql(\"SELECT geohash, sum, cnt, (sum / cnt) AS div \\\n",
    "            FROM ( \\\n",
    "                SELECT geohash, SUM(categorical_snow_yes1_no0_surface) AS sum, COUNT(*) as cnt \\\n",
    "                FROM nam_s \\\n",
    "                GROUP BY(geohash) \\\n",
    "            ) ORDER BY div DESC\")\n",
    "so_snowy_s.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(geohash='c41uhb4r5n00', sum=168.0, cnt=421, div=0.3990498812351544),\n",
       " Row(geohash='c45277s4gjpb', sum=163.0, cnt=423, div=0.38534278959810875),\n",
       " Row(geohash='c44jc11cn1rz', sum=165.0, cnt=433, div=0.3810623556581986),\n",
       " Row(geohash='c41yek3dwk2p', sum=152.0, cnt=403, div=0.3771712158808933),\n",
       " Row(geohash='c1uz20wg2gxb', sum=141.0, cnt=376, div=0.375),\n",
       " Row(geohash='c439n53vsxzz', sum=155.0, cnt=417, div=0.37170263788968827),\n",
       " Row(geohash='c1gv86v08280', sum=148.0, cnt=401, div=0.3690773067331671),\n",
       " Row(geohash='c1gy5p6c9n2p', sum=150.0, cnt=407, div=0.36855036855036855),\n",
       " Row(geohash='c41uxkww12rz', sum=148.0, cnt=403, div=0.36724565756823824),\n",
       " Row(geohash='c1gru77j5fzz', sum=167.0, cnt=455, div=0.367032967032967),\n",
       " Row(geohash='c44srx2d2tzz', sum=158.0, cnt=431, div=0.3665893271461717),\n",
       " Row(geohash='c43kcu3t702p', sum=149.0, cnt=409, div=0.3643031784841076),\n",
       " Row(geohash='c43k6uu1egxb', sum=150.0, cnt=412, div=0.3640776699029126),\n",
       " Row(geohash='c44j614gx8xb', sum=141.0, cnt=389, div=0.36246786632390743),\n",
       " Row(geohash='c438x5esgf00', sum=143.0, cnt=395, div=0.3620253164556962),\n",
       " Row(geohash='c43hr0kbbvpb', sum=158.0, cnt=437, div=0.36155606407322655),\n",
       " Row(geohash='c1vqm5v4umpb', sum=155.0, cnt=430, div=0.36046511627906974),\n",
       " Row(geohash='c432ydkuzy2p', sum=145.0, cnt=403, div=0.3598014888337469),\n",
       " Row(geohash='c41v48pupf00', sum=155.0, cnt=431, div=0.35962877030162416),\n",
       " Row(geohash='c438fqgmsm00', sum=157.0, cnt=438, div=0.3584474885844749),\n",
       " Row(geohash='c43h57tx92xb', sum=140.0, cnt=391, div=0.35805626598465473),\n",
       " Row(geohash='c41z4k0q1v00', sum=136.0, cnt=380, div=0.35789473684210527),\n",
       " Row(geohash='c41vnktkn7xb', sum=151.0, cnt=424, div=0.3561320754716981),\n",
       " Row(geohash='c43kp9tv6krz', sum=141.0, cnt=396, div=0.3560606060606061),\n",
       " Row(geohash='c41ueb1jyypb', sum=143.0, cnt=402, div=0.35572139303482586),\n",
       " Row(geohash='c1vrpbjs6180', sum=133.0, cnt=374, div=0.35561497326203206),\n",
       " Row(geohash='c452rpbu2mbp', sum=143.0, cnt=403, div=0.3548387096774194),\n",
       " Row(geohash='c41vtks3952p', sum=143.0, cnt=404, div=0.35396039603960394),\n",
       " Row(geohash='c450pxr2zbxb', sum=139.0, cnt=393, div=0.35368956743002544),\n",
       " Row(geohash='c3041ty0ftpb', sum=146.0, cnt=414, div=0.3526570048309179),\n",
       " Row(geohash='c43s5qjhy0xb', sum=139.0, cnt=395, div=0.3518987341772152),\n",
       " Row(geohash='c1gvsp10dmzz', sum=158.0, cnt=449, div=0.3518930957683742),\n",
       " Row(geohash='c432rdwup080', sum=139.0, cnt=397, div=0.3501259445843829),\n",
       " Row(geohash='c41z9hnc43rz', sum=142.0, cnt=407, div=0.3488943488943489),\n",
       " Row(geohash='c4459s8rz1xb', sum=137.0, cnt=393, div=0.3486005089058524),\n",
       " Row(geohash='c43b05v7222p', sum=137.0, cnt=393, div=0.3486005089058524),\n",
       " Row(geohash='c4qm97202jup', sum=136.0, cnt=392, div=0.3469387755102041),\n",
       " Row(geohash='c41yhk7n3n00', sum=133.0, cnt=386, div=0.344559585492228),\n",
       " Row(geohash='c1gwp7n9ecbp', sum=152.0, cnt=442, div=0.3438914027149321),\n",
       " Row(geohash='c1vx5s4d8u7z', sum=131.0, cnt=381, div=0.3438320209973753),\n",
       " Row(geohash='c1tn7vh1phxb', sum=142.0, cnt=413, div=0.34382566585956414),\n",
       " Row(geohash='c1grzzxwt1bp', sum=143.0, cnt=416, div=0.34375),\n",
       " Row(geohash='c41x3g48q200', sum=135.0, cnt=393, div=0.3435114503816794),\n",
       " Row(geohash='c1vw2zccus80', sum=139.0, cnt=405, div=0.3432098765432099),\n",
       " Row(geohash='c44s7ffyr9rz', sum=141.0, cnt=411, div=0.34306569343065696),\n",
       " Row(geohash='c41wrxgfefpb', sum=136.0, cnt=397, div=0.3425692695214106),\n",
       " Row(geohash='c437gvnt3d00', sum=138.0, cnt=403, div=0.3424317617866005),\n",
       " Row(geohash='c41xmx88zn80', sum=136.0, cnt=398, div=0.3417085427135678),\n",
       " Row(geohash='c453n01c9exb', sum=142.0, cnt=416, div=0.34134615384615385),\n",
       " Row(geohash='c43sdmg10700', sum=130.0, cnt=381, div=0.34120734908136485),\n",
       " Row(geohash='c41xurr50ypb', sum=139.0, cnt=409, div=0.33985330073349634),\n",
       " Row(geohash='c44uh5wjf92p', sum=140.0, cnt=412, div=0.33980582524271846),\n",
       " Row(geohash='c4387rjkb0rz', sum=149.0, cnt=439, div=0.33940774487471526),\n",
       " Row(geohash='c37rkxnm3cup', sum=133.0, cnt=392, div=0.3392857142857143),\n",
       " Row(geohash='c1p7nnvm1g5b', sum=144.0, cnt=425, div=0.3388235294117647),\n",
       " Row(geohash='c1sgmt7hzg80', sum=132.0, cnt=390, div=0.3384615384615385),\n",
       " Row(geohash='c1uyc0k42crz', sum=136.0, cnt=405, div=0.3358024691358025),\n",
       " Row(geohash='c44hk1hy9u2p', sum=138.0, cnt=411, div=0.3357664233576642),\n",
       " Row(geohash='c1gr1wvsqzrz', sum=139.0, cnt=414, div=0.3357487922705314),\n",
       " Row(geohash='c1grm74n9f00', sum=139.0, cnt=414, div=0.3357487922705314),\n",
       " Row(geohash='c37p9qfx2x80', sum=140.0, cnt=417, div=0.33573141486810554),\n",
       " Row(geohash='c4393q9myw00', sum=142.0, cnt=424, div=0.33490566037735847),\n",
       " Row(geohash='c44eufgnnq2p', sum=132.0, cnt=395, div=0.3341772151898734),\n",
       " Row(geohash='c44hg15vqhxb', sum=135.0, cnt=404, div=0.3341584158415842),\n",
       " Row(geohash='c433md4vjx80', sum=136.0, cnt=408, div=0.3333333333333333),\n",
       " Row(geohash='fd9fuvhc2sxb', sum=133.0, cnt=399, div=0.3333333333333333),\n",
       " Row(geohash='c4n2kr7y2rs0', sum=133.0, cnt=399, div=0.3333333333333333),\n",
       " Row(geohash='c1t540c4vjh0', sum=138.0, cnt=415, div=0.3325301204819277),\n",
       " Row(geohash='c44syx23gepb', sum=135.0, cnt=406, div=0.33251231527093594),\n",
       " Row(geohash='c1t384wx6srz', sum=130.0, cnt=391, div=0.33248081841432225),\n",
       " Row(geohash='fd9gmu8sjuup', sum=144.0, cnt=434, div=0.3317972350230415),\n",
       " Row(geohash='c1vmy53xcjzz', sum=139.0, cnt=419, div=0.3317422434367542),\n",
       " Row(geohash='fd9g109d19gz', sum=134.0, cnt=404, div=0.3316831683168317),\n",
       " Row(geohash='c1vqb85vdvh0', sum=136.0, cnt=412, div=0.3300970873786408),\n",
       " Row(geohash='c4n22ewhww00', sum=135.0, cnt=409, div=0.33007334963325186),\n",
       " Row(geohash='c1nrwpc5kpkp', sum=150.0, cnt=455, div=0.32967032967032966),\n",
       " Row(geohash='c1th060z91up', sum=142.0, cnt=431, div=0.3294663573085847),\n",
       " Row(geohash='c445v1jx5z2p', sum=138.0, cnt=419, div=0.32935560859188545),\n",
       " Row(geohash='c3k396yhyuup', sum=132.0, cnt=401, div=0.32917705735660846),\n",
       " Row(geohash='c4j7z4pc5rxb', sum=132.0, cnt=401, div=0.32917705735660846),\n",
       " Row(geohash='c1t3hw59skzz', sum=136.0, cnt=416, div=0.3269230769230769),\n",
       " Row(geohash='c43mj8cjkdpb', sum=127.0, cnt=389, div=0.3264781491002571),\n",
       " Row(geohash='c44v95tupg80', sum=135.0, cnt=414, div=0.32608695652173914),\n",
       " Row(geohash='c1vws7y8prbp', sum=131.0, cnt=402, div=0.32587064676616917),\n",
       " Row(geohash='c1gx2zw2p3bp', sum=139.0, cnt=428, div=0.3247663551401869),\n",
       " Row(geohash='c4qfp6fm69bp', sum=134.0, cnt=413, div=0.324455205811138),\n",
       " Row(geohash='c3k2d8k3h7pb', sum=133.0, cnt=410, div=0.32439024390243903),\n",
       " Row(geohash='c3k91p7mjeh0', sum=130.0, cnt=401, div=0.32418952618453867),\n",
       " Row(geohash='c1gr8xnduyrz', sum=128.0, cnt=395, div=0.3240506329113924),\n",
       " Row(geohash='c44emfgm6r2p', sum=136.0, cnt=420, div=0.3238095238095238),\n",
       " Row(geohash='c3k89wn4zes0', sum=135.0, cnt=417, div=0.3237410071942446),\n",
       " Row(geohash='c43m2snbpzbp', sum=133.0, cnt=411, div=0.3236009732360097),\n",
       " Row(geohash='c4qsrnzct8kp', sum=131.0, cnt=405, div=0.3234567901234568),\n",
       " Row(geohash='c36m6mkpn3rz', sum=130.0, cnt=402, div=0.32338308457711445),\n",
       " Row(geohash='c4jk4whz827z', sum=129.0, cnt=399, div=0.3233082706766917),\n",
       " Row(geohash='c43j14zyhszz', sum=134.0, cnt=415, div=0.3228915662650602),\n",
       " Row(geohash='c4qupwjv0mgz', sum=138.0, cnt=428, div=0.32242990654205606),\n",
       " Row(geohash='c37r3h5gq7eb', sum=137.0, cnt=425, div=0.32235294117647056),\n",
       " Row(geohash='c3e028j8ggkp', sum=137.0, cnt=425, div=0.32235294117647056),\n",
       " Row(geohash='c3k508c9y1s0', sum=126.0, cnt=391, div=0.32225063938618925)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "so_snowy_s.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "so_snowy = spark.sql(\"SELECT geohash, sum, cnt, (sum / cnt) AS div \\\n",
    "            FROM ( \\\n",
    "                SELECT geohash, SUM(categorical_snow_yes1_no0_surface) AS sum, COUNT(*) as cnt \\\n",
    "                FROM nam \\\n",
    "                GROUP BY(geohash) \\\n",
    "            ) \\\n",
    "            ORDER BY div DESC \\\n",
    "            LIMIT 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o283.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 37.0 failed 4 times, most recent failure: Lost task 1.3 in stage 37.0 (TID 937, 10.0.1.21, executor 2): java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:314)\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n\tat java.nio.channels.Channels.access$000(Channels.java:61)\n\tat java.nio.channels.Channels$1.write(Channels.java:174)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:174)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1101)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:68)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1029)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1433)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1420)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:135)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:314)\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n\tat java.nio.channels.Channels.access$000(Channels.java:61)\n\tat java.nio.channels.Channels$1.write(Channels.java:174)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:174)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1101)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:68)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-6a08393cee46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mso_snowy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1132\u001b[0m             \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \"\"\"\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.3.1-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o283.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 37.0 failed 4 times, most recent failure: Lost task 1.3 in stage 37.0 (TID 937, 10.0.1.21, executor 2): java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:314)\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n\tat java.nio.channels.Channels.access$000(Channels.java:61)\n\tat java.nio.channels.Channels$1.write(Channels.java:174)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:174)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1101)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:68)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2131)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1029)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1433)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1420)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:135)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:65)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat org.apache.spark.storage.CountingWritableChannel.write(DiskStore.scala:314)\n\tat java.nio.channels.Channels.writeFullyImpl(Channels.java:78)\n\tat java.nio.channels.Channels.writeFully(Channels.java:101)\n\tat java.nio.channels.Channels.access$000(Channels.java:61)\n\tat java.nio.channels.Channels$1.write(Channels.java:174)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeByte(ObjectOutputStream.java:1915)\n\tat java.io.ObjectOutputStream.writeFatalException(ObjectOutputStream.java:1576)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:351)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:140)\n\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:174)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1101)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$7.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:68)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1099)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1018)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1083)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:809)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "so_snowy.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strangely Snowy \n",
    "Find a location that contains snow while its surroundings do not. Why does this occur? Is it a high mountain peak in a desert?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning rod\n",
    "Where are you most likely to be struck by lightning? Use a precision of at least 4 Geohash characters and provide the top 3 locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_rod = spark.sql(\"\\\n",
    "                           SELECT LEFT(geohash, 4) AS geo4, SUM(lightning_surface) / COUNT(*) AS prob_lightning \\\n",
    "                           FROM nam_tiny \\\n",
    "                           GROUP BY geo4 \\\n",
    "                           ORDER BY prob_lightning DESC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(geo4='dmbz', prob_lightning=1.0),\n",
       " Row(geo4='c4xg', prob_lightning=0.0),\n",
       " Row(geo4='d5x3', prob_lightning=0.0)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning_rod.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drying out\n",
    "Choose a region in North America (defined by one or more Geohashes) and determine when its driest month is. This should include a histogram with data from each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_unixtime(timestamp DIV 1000) as timestring, \\\n",
    "\n",
    "drying_out = spark.sql(\"\\\n",
    "    SELECT \\\n",
    "        trunc(from_unixtime(timestamp DIV 1000), 'MM') as truncated \\\n",
    "    FROM nam_tiny \\\n",
    "    GROUP BY truncated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| truncated|\n",
      "+----------+\n",
      "|2015-03-01|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drying_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travel Startup\n",
    "After graduating from USF, you found a startup that aims to provide personalized travel itineraries using big data analysis. Given your own personal preferences, build a plan for a year of travel across 5 locations. Or, in other words: pick 5 regions. What is the best time of year to visit them based on the dataset?\n",
    "\n",
    "- One avenue here could be determining the comfort index for a region. You could incorporate several features: not too hot, not too cold, dry, humid, windy, etc. There are several different ways of calculating this available online, and you could also analyze how well your own metrics do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escaping the fog\n",
    "After becoming rich from your startup, you are looking for the perfect location to build your Bay Area mansion with unobstructed views. Find the locations that are the least foggy and show them on a map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\\\n",
    "    SELECT LEFT(geohash) as geo4, SUM(visibility_surface) /  \\\n",
    "    FROM nam_tiny \\\n",
    "    GROUP BY truncated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SolarWind, Inc.\n",
    "You get bored enjoying the amazing views from your mansion, so you start a new company; here, you want to help power companies plan out the locations of solar and wind farms across North America. Locate the top 3 places for solar and wind farms, as well as a combination of both (solar + wind farm). You will report a total of 9 Geohashes as well as their relevant attributes (for example, cloud cover and wind speeds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even create a sample dataset with Spark! Let's create a 10% sample (without replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = df.sample(False, .1)\n",
    "\n",
    "# Write it out to a file\n",
    "samp.write.format('csv').save('hdfs://orion12:50000/sampled_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayarea_tiny = spark.sql(\"\\\n",
    "                           SELECT * \\\n",
    "                           FROM nam_tiny \\\n",
    "                           WHERE LEFT(geohash, 3) IN ('9qb', '9qc', '9q9', '9q8')\")\n",
    "\n",
    "\n",
    "nine_tiny = spark.sql(\"\\\n",
    "                           SELECT * \\\n",
    "                           FROM nam_tiny \\\n",
    "                           WHERE LEFT(geohash, 1) IN ('9', '8')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayarea_small = spark.sql(\"\\\n",
    "                           SELECT * \\\n",
    "                           FROM nam_small \\\n",
    "                           WHERE LEFT(geohash, 3) IN ('9qb', '9qc', '9q9', '9q8')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayarea_full = spark.sql(\"\\\n",
    "SELECT * \\\n",
    "FROM nam_full \\\n",
    "WHERE LEFT(geohash, 4) IN ( \\\n",
    " '9q8x', '9q8z', '9q8y', '9q8v', '9q8u', '9q8g', \\\n",
    " '9q9p', '9q9n', '9q9j', '9q9h', '9q95', '9q9r', '9q9q', '9q9m', '9q9k', '9q97', '9q9e', \\\n",
    " '9qb3', '9q95', '9qb6', '9qb7', '9qb8', '9qb9', '9qbb', '9qbc', '9qbd', '9qbe', '9qbf', '9qbg', '9qbh', '9qbk', '9qbs', \\\n",
    " '9qc0', '9qc1', '9qc2', '9qc3', '9qc4', '9qc5', '9qc6', '9qch' \\\n",
    ")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayarea_full.write.format('csv').save('hdfs://orion11:15000/nam_bayarea')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayarea_full.createOrReplaceTempView(\"nam_bayarea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
